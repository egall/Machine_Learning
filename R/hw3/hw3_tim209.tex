\documentclass{article}

\usepackage[latin1]{inputenc}
\usepackage{times,fullpage,amsmath}
\usepackage{enumerate,graphicx,hyperref,verbatim, amsmath, mathtools, pdfpages}
\DeclareMathSizes{10}{10}{10}{10}
\title{TIM209 Homework \#3}
\author{Erik Steggall \\ esteggall@soe.ucsc.edu}

\date{Fall 2014}
\setlength\parindent{0pt}
\begin{document} \maketitle \pagestyle{empty}
\section*{Problem 1}
\subsection*{a}
We will use the formula:\\
\begin{align}
P(x) =\frac{ e^{\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}}}{1 + e^{\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}}}\\\\
P(x) =\frac{ e^{(-6)+(0.05)(40)+(1)(3.5)}}{1 + e^{(-6)+(0.05)(40)+(1)(3.5)}}\\\\
P(x) =\frac{ e^{-0.5}}{1 + e^{-0.5}}\\\\
P(x) = 0.3775407\\
\end{align}
\subsection*{b}
Starting with the same formula, we plug in 0.5 for P(x) this time, leaving out $X_{1}$ to get:\\
\begin{align}
P(x) =\frac{ e^{\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}}}{1 + e^{\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}}}\\\\
0.5 = P(x) =\frac{ e^{(-6)+(0.05)X_{1}+(1)(3.5)}}{1 + e^{(-6)+(0.05)X_{1}+(1)(3.5)}}\\\\
0.50 (1 + e^{-2.5 + 0.05 X_1}) = e^{-2.5 + 0.05 X_1} \\ \\
0.50 + 0.50(e^{-2.5 + 0.05 X_1}) = e^{-2.5 + 0.05 X_1}\\\\
0.50 = 0.50(e^{-2.5 + 0.05 X_1}) \\\\
\log(1) = -2.5 + 0.05 X_1\\\\
X_1 = 2.5 / 0.05 = 50\\\\
\end{align}
So It will take 50 hours to get an A.\\


\section*{Problem 2}
We start with the generic equation for LDA:\\\\
$p_k(x) = \frac {\pi_k \frac {1} {\sqrt{2 \pi} \sigma} exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)}  {\sum\limits_{l=1}^K { \pi_l \frac {1} {\sqrt{2 \pi} \sigma} exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }}$ \\ \\
Next we do a bit of factoring to simplify the equation:\\\\
$ \frac {\pi_{yes} \exp(- \frac {1} {2 \sigma^2} (x - \mu_{yes})^2) } {\sum\limits_{l=1}^K { \pi_l \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }}$ \\\\
We expand the summation into the two possibilities, yes and no:\\\\
$\frac {\pi_{yes} \exp(- \frac {1} {2 \sigma^2} (x - \mu_{yes})^2)} { \pi_{yes} \exp(- \frac {1} {2 \sigma^2} (x - \mu_{yes})^2) + \pi_{no} \exp(- \frac {1} {2 \sigma^2} (x - \mu_{no})^2) }$\\\\
Next, we sub in our values for $\sigma^2 = 36$ and $\pi_{yes} = 0.80, \pi_{no} = 0.20$ and $\mu_{yes} = 10,  \mu_{no} = 0$ and $X = 4$\\\\
$p_{yes}(4) = \frac {0.80 \exp(- \frac {1} {2 * 36} (4 - 10)^2)} { 0.80 \exp(- \frac {1} {2 * 36} (4 - 10)^2) + 0.20 \exp(- \frac {1} {2 * 36} 4^2) }$ \\\\    
We now have to do a bit of math to get:\\\\
$ = \frac {0.80 \exp(- \frac {1} {2*36} (36))} { 0.80 \exp(- \frac {1} {2*36} (36)) + 0.20 \exp(- \frac {1} {2*36} 16) }$\\\\\\

$ = \frac {0.80 \exp(- \frac {1} {2})} { 0.80 \exp(- \frac {1} {2}) + 0.20 \exp(- \frac {16} {72}) } = 75.18\%$\\\\

\section*{Problem 3}

\subsection*{a}
\begin{verbatim}
> mpg01
  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0
 [38] 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 [75] 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0
[112] 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
[149] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1
[186] 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0
[223] 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0
[260] 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1
[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1
[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1
\end{verbatim}
\subsection*{b}
It's interesting to look at the scatterplots of mpg vs other variables. I found that cylinders, year, and origin were all linearly separable. Whereas displacement, horsepower, weight, and acceleration did not have as strong a correlation, but they were very similar to one another. This was interesting to me because I would figure that displacement, horsepower, weight, and acceleration would be highly correlated to mpg, whereas the other variables; cylinders, year, and origin had a much stronger relationship.\\
I provided an example of mpg vs horsepower, and the second plot is mpg vs cylinders to illustrate the two different graphs:\\

\includepdf[pages={1}]{mpgvhp.pdf}
\includepdf[pages={1}]{mpgvcylinders.pdf}

\subsection*{d}
I got a mean = 0.1530612.\\

\subsection*{e}
I couldn't get QDA to work. I keep getting the error:\\
\begin{verbatim}
Error in qda.default(x, grouping, ...) : rank deficiency in group 1
\end{verbatim}

\subsection*{f}
I got the same mean as LDA, mean = 0.1530612.\\


\subsection*{g}
I couldn't get KKN to work either. Got the error:\\
\begin{verbatim}
Error in eval(expr, envir, enclos) : could not find function "kkn"
\end{verbatim}

\section*{Second Part}

\section*{Problem 1}
You don't have the same error rate for logistic functions because logistic functions are heteroscadastic, meaning the error rate will have a different meaning depending on where it is on the graph. Logistic regression deals with probabilities, so the error term is implicit in the answer.\\

\section*{Problem 2}
The X would not be random. It wouldn't make sense to treat X as a random variable because it would throw off our entire reasoning behind using linear regression. Our X's can be given randomness by adding in error, which will make our X deviate depending on the Gaussian distribution.\\

\section*{Problem 3}
It doesn't make sense to have a logistic function predict anywhere outside of 0 and 1, because it predicts based on probabilities. It doesn't make sense to have a probability outside of the range of 0 and 1 because you can't have a negative probability, or a probability above 100\%.\\

\section*{Problem 4}
\subsection*{i}
We would need to add in exponents to make the equation some form of polynomial.\\
\subsection*{ii}

\subsection*{iii}
The heteroscadacity can be identified by viewing the residual plot. If the residual plot is in a funnel shape. The solution is to transform the response of Y into a concave function, this can be done by doing either log(y) or taking the the square root of Y. This reduces the variance of the residuals.\\
\subsection*{iv}
An outlier is defined as a point that is significantly dissimilar from the rest of the data. A high leverage point is an outlier that effects the slope of the linear regression significantly, thus, most outliers are high leverage points, but this is not necessarily true.\\




\end{document}