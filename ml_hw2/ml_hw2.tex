\documentclass{article}

\usepackage[latin1]{inputenc}
\usepackage{times,fullpage,amsmath}
\usepackage{enumerate,graphicx,hyperref,verbatim, amsmath, mathtools}

\title{CS242 Homework \#1}
\author{Erik Steggall, Jay Kim, Rakshit Agrawal \\ esteggall@soe.ucsc.edu, jaykim@soe.ucsc.edu, ragrawa1@ucsc.edu}

\date{Fall 2014}
\setlength\parindent{0pt}
\begin{document} \maketitle \pagestyle{empty}
\section*{Problem 1}

\subsection*{'+' '+'}
For the cases of '+' being predicted and '+' being chosen:\\ 
+++ and ++- and +-+ and -++\\
Which is:\\
\begin{align}
\frac{2}{3}((\frac{2}{3}) + 3 (\frac{1}{3} * (\frac{2}{3}^2)))\\
\frac{2}{3}(\frac{16}{27} + 3(\frac{4}{27}))\\
\frac{2}{3}(\frac{28}{27}) = \frac{56}{81}\\
\end{align}
\subsection*{'+' '-'}
For the case of '-' being predicted and '+' being chosen we have 4 cases:\\
+++ and ++- and +-+ and -++\\
Which is:\\
\begin{align}
\frac{1}{3}((\frac{2}{3}) + 3 (\frac{1}{3} * (\frac{2}{3}^2)))\\
\frac{1}{3}(\frac{16}{27} + 3(\frac{4}{27}))\\
\frac{1}{3}(\frac{28}{27}) = \frac{28}{81}\\
\end{align}

\subsection*{'-' '+'}
For the case of '+' being predicted and '-'  being chosen:\\
Which is:\\
\begin{align}
\frac{2}{3}((\frac{1}{3}) + 3 (\frac{2}{3} * (\frac{1}{3}^2)))\\
\frac{2}{3}(\frac{1}{27} + 3(\frac{2}{27}))\\
\frac{2}{3}(\frac{7}{27}) = \frac{14}{81}\\
\end{align}

\subsection*{'-' '-'}
For the case of '-' being predicted and '-' being chosen we have 4 cases also:\\
\begin{align}
\frac{1}{3}((\frac{1}{3}) + 3 (\frac{2}{3} * (\frac{1}{3}^2)))\\
\frac{1}{3}(\frac{1}{27} + 3(\frac{2}{27}))\\
\frac{1}{3}(\frac{7}{27}) = \frac{7}{81}\\
\end{align}

\subsection*{Average case}
$ \frac{56/81 + 28/81 + 14/81 + 7/81}{4}  = \frac{32}{108}$


\section*{Problem 2}
\subsection*{a}
The data has a binary separation for each feature, so if it takes on the label of it's neighbor than it is correct.\\
\subsection*{b}
The accuracy goes from 100\% to 70.18\%. 
\subsection*{c}
In the general case I would expect the greater the K value means a tradeoff between more computation but a more accurate the result. This is what I see in this case, where we get 70.18\% accuracy with $k = 1$, 72.65\% accuracy with $k = 2$ and 73.17\% accuracy with $k = 5$.\\
\subsection*{d}
I chose to add another 10 versions of class, which had either positive or negative valued entries. This resulted in 100\% prediction accuracy. I suspect that this flaw, having too many irrelevant attributes, is one of major drawbacks of KNN.\\
\subsection*{e}
The accuracy dropped to 52.7344\%, this is surprising because it means that the $\frac{1}{3}$ which are legitmate attributes only count for ~2\% of the prediction accuracy.\\
\subsection*{f}
It appears from the last two tests that Weka does normalize the attributes. Both of these tests are good examples of this.\\


\end{document}